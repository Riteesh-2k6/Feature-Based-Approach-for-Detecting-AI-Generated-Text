import streamlit as st
import torch
import pandas as pd
import numpy as np
from feature_extractor import LinguisticFeatureExtractor
from hybrid_model import HybridAIDetector
import plotly.graph_objects as go
import nltk

# Ensure NLTK data is available
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Page Config
st.set_page_config(
    page_title="Hybrid AI Text Detector",
    page_icon="ü§ñ",
    layout="wide"
)

# Title and Description
st.title("ü§ñ Hybrid AI vs Human Text Detector")
st.markdown("""
This tool uses a hybrid model combining **linguistic feature extraction** and **neural embeddings** 
to detect whether a piece of text was written by a human or generated by AI.
""")

# Sidebar
st.sidebar.header("Model Configuration")
device = st.sidebar.selectbox(
    "Device",
    ["cuda" if torch.cuda.is_available() else "cpu", "cpu"],
    index=0
)

@st.cache_resource
def load_model_and_extractor(device_name):
    """Loads the model and feature extractor (cached)"""
    
    # Load Feature Extractor
    with st.spinner("Loading Feature Extractor..."):
        extractor = LinguisticFeatureExtractor()
        
    # Load Model
    with st.spinner("Loading Neural Model..."):
        model = HybridAIDetector(feature_dim=54)
        model_path = "models/best_model.pt"
        
        try:
            checkpoint = torch.load(model_path, map_location=device_name)
            model.load_state_dict(checkpoint["model_state_dict"])
            model.to(device_name)
            model.eval()
        except FileNotFoundError:
            st.error(f"Model file not found at {model_path}. Please train the model first.")
            return None, None
        except Exception as e:
            st.error(f"Error loading model: {e}")
            return None, None
            
    return model, extractor

# Load resources
model, feature_extractor = load_model_and_extractor(device)

if model and feature_extractor:
    st.sidebar.success("Model loaded successfully!")
    
    # Input Area
    input_text = st.text_area("Enter text to analyze:", height=200, placeholder="Paste article, essay, or email content here...")
    
    col1, col2 = st.columns([1, 5])
    with col1:
        analyze_btn = st.button("üîç Analyze Text", type="primary")
    
    if analyze_btn and input_text:
        if len(input_text.strip()) < 10:
            st.warning("Please enter a longer text sample (at least 10 characters).")
        else:
            try:
                with st.spinner("Analyzing..."):
                    # --- Global Analysis ---
                    # 1. Extract Features (Dictionary for display)
                    features_dict = feature_extractor.extract_all_features(input_text, return_dict=True)
                    
                    # 2. Prepare for Model
                    # The model expects a raw numpy array for the linguistic path
                    features_array = feature_extractor.extract_all_features(input_text, return_dict=False)
                    ling_features_tensor = torch.FloatTensor(features_array).unsqueeze(0).to(device)
                    
                    # 3. Predict
                    predictions, confidences = model.predict(ling_features_tensor, [input_text], device=device)
                    
                    is_ai = predictions.item() == 1
                    confidence = confidences.item()
                    ai_prob = confidence if is_ai else 1 - confidence
                    human_prob = 1 - ai_prob

                # --- Display Results ---
                st.divider()
                
                # Main Result Header
                result_col1, result_col2 = st.columns([2, 3])
                
                with result_col1:
                    if is_ai:
                        st.error("üö® AI-Generated Content Detected")
                    else:
                        st.success("‚úÖ Human-Written Content Detected")
                        
                    # Gauge Chart
                    fig = go.Figure(go.Indicator(
                        mode = "gauge+number",
                        value = ai_prob * 100,
                        title = {'text': "Overall AI Probability"},
                        gauge = {
                            'axis': {'range': [0, 100]},
                            'bar': {'color': "red" if is_ai else "green"},
                            'steps': [
                                {'range': [0, 50], 'color': "#f0f2f6"},
                                {'range': [50, 100], 'color': "#ffebee"} if is_ai else {'range': [0, 50], 'color': "#e8f5e9"}
                            ],
                            'threshold': {
                                'line': {'color': "black", 'width': 4},
                                'thickness': 0.75,
                                'value': 50
                            }
                        }
                    ))
                    fig.update_layout(height=300, margin=dict(l=20, r=20, t=50, b=20))
                    st.plotly_chart(fig, use_container_width=True)

                with result_col2:
                    st.subheader("Analysis Details")
                    st.info(f"**Confidence:** {confidence:.2%}")
                    
                    st.write("The model analyzes syntactic patterns, lexical diversity, and perplexity to determine the origin of the text.")
                    
                    # Feature Highlights
                    st.caption("Key Linguistic Metrics")
                    
                    key_metrics = {k: v for k, v in features_dict.items() if any(x in k.lower() for x in ['entropy', 'diversity', 'flesch', 'perplexity', 'ttr'])}
                    if not key_metrics:
                         key_metrics = dict(list(features_dict.items())[:5]) # Fallback
                    
                    metrics_df = pd.DataFrame(list(key_metrics.items()), columns=["Metric", "Value"])
                    st.dataframe(metrics_df, hide_index=True, use_container_width=True)

                # Expandable full features
                with st.expander("üìÇ View All Extracted Linguistic Features"):
                    st.json(features_dict)

            except Exception as e:
                st.error(f"An error occurred during analysis: {e}")
                st.exception(e)

else:
    st.info("Waiting for model to load...")
